"use strict";(self.webpackChunkcreate_project_docs=self.webpackChunkcreate_project_docs||[]).push([[3791],{28453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var r=s(96540);const i={},t=r.createContext(i);function o(e){const n=r.useContext(t);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(t.Provider,{value:n},e.children)}},48760:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"system-architecture/design","title":"Design Document - Part I: Architecture","description":"Purpose","source":"@site/docs/system-architecture/design.md","sourceDirName":"system-architecture","slug":"/system-architecture/design","permalink":"/project-002-aac-api/docs/system-architecture/design","draft":false,"unlisted":false,"editUrl":"https://github.com/Capstone-Projects-2025-Fall/project-002-aac-api/edit/main/documentation/docs/system-architecture/design.md","tags":[],"version":"current","lastUpdatedBy":"AceShadowKnight","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docsSidebar","previous":{"title":"System Architecture","permalink":"/project-002-aac-api/docs/category/system-architecture"},"next":{"title":"Development Environment","permalink":"/project-002-aac-api/docs/system-architecture/development-environment"}}');var i=s(74848),t=s(28453);const o={sidebar_position:1},a="Design Document - Part I: Architecture",d={},l=[{value:"Purpose",id:"purpose",level:2},{value:"System Architecture Overview",id:"system-architecture-overview",level:2},{value:"Key Features (v2.0.0)",id:"key-features-v200",level:3},{value:"API Endpoints Overview",id:"api-endpoints-overview",level:2},{value:"Component Architecture",id:"component-architecture",level:2},{value:"1. Client Layer (Game Application)",id:"1-client-layer-game-application",level:3},{value:"2. Server Layer (Express.js)",id:"2-server-layer-expressjs",level:3},{value:"3. Audio Processing Layer (Python)",id:"3-audio-processing-layer-python",level:3},{value:"4. External Service Layer",id:"4-external-service-layer",level:3},{value:"Data Flow Diagrams",id:"data-flow-diagrams",level:2},{value:"Successful Request Flow",id:"successful-request-flow",level:3},{value:"Error Handling Flow",id:"error-handling-flow",level:3},{value:"Complete Use Case: AAC User Playing TicTacToe",id:"complete-use-case-aac-user-playing-tictactoe",level:3},{value:"Health Check Flow",id:"health-check-flow",level:3},{value:"Class Diagrams",id:"class-diagrams",level:2},{value:"Express Server Architecture",id:"express-server-architecture",level:3},{value:"Python Module Architecture",id:"python-module-architecture",level:3},{value:"Client Integration Module",id:"client-integration-module",level:3},{value:"Database Design",id:"database-design",level:2},{value:"Log File Structure",id:"log-file-structure",level:3},{value:"Algorithms and Processing Logic",id:"algorithms-and-processing-logic",level:2},{value:"1. Audio Format Detection Algorithm",id:"1-audio-format-detection-algorithm",level:3},{value:"2. Audio Quality Validation Algorithm",id:"2-audio-quality-validation-algorithm",level:3},{value:"3. Audio Preprocessing Algorithm",id:"3-audio-preprocessing-algorithm",level:3},{value:"4. Command Classification Algorithm",id:"4-command-classification-algorithm",level:3},{value:"5. User Agent Parsing Algorithm",id:"5-user-agent-parsing-algorithm",level:3},{value:"6. Confidence-Based Command Matching",id:"6-confidence-based-command-matching",level:3},{value:"Security Considerations",id:"security-considerations",level:2},{value:"1. Input Validation",id:"1-input-validation",level:3},{value:"2. Data Privacy",id:"2-data-privacy",level:3},{value:"3. API Security",id:"3-api-security",level:3},{value:"4. Subprocess Security",id:"4-subprocess-security",level:3},{value:"5. Recommended Production Hardening",id:"5-recommended-production-hardening",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"1. Memory Usage",id:"1-memory-usage",level:3},{value:"2. Concurrency",id:"2-concurrency",level:3},{value:"3. Response Times",id:"3-response-times",level:3},{value:"4. Optimization Strategies",id:"4-optimization-strategies",level:3},{value:"Deployment Architecture",id:"deployment-architecture",level:2},{value:"Development Environment",id:"development-environment",level:3},{value:"Production Environment (Recommended)",id:"production-environment-recommended",level:3},{value:"Docker Deployment",id:"docker-deployment",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Architectural Decisions",id:"key-architectural-decisions",level:3},{value:"Version History",id:"version-history",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"design-document---part-i-architecture",children:"Design Document - Part I: Architecture"})}),"\n",(0,i.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,i.jsx)(n.p,{children:"The Design Document - Part I Architecture describes the software architecture and how the requirements are mapped into the design. This document combines diagrams and text that describes the system's components, their interactions, and the data flow through the AAC Integration API."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"system-architecture-overview",children:"System Architecture Overview"}),"\n",(0,i.jsxs)(n.p,{children:["The AAC Integration API follows a ",(0,i.jsx)(n.strong,{children:"client-server architecture"})," with a ",(0,i.jsx)(n.strong,{children:"Python subprocess pipeline"})," for audio processing. The system is designed to be stateless, processing each audio upload request independently while maintaining optional logging capabilities for analytics."]}),"\n",(0,i.jsx)(n.h3,{id:"key-features-v200",children:"Key Features (v2.0.0)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multiple Recognition Engines"})," - Google Speech Recognition with Vosk offline fallback"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AAC Command Mode"})," - Optimized recognition for short commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Word-Level Timing"})," - Start/end times for each recognized word"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Standardized Responses"})," - Consistent camelCase JSON format"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Health Monitoring"})," - Server status and service availability endpoints"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Confidence Scoring"})," - Recognition confidence for filtering results"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"api-endpoints-overview",children:"API Endpoints Overview"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Method"}),(0,i.jsx)(n.th,{children:"Endpoint"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"GET"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/health"})}),(0,i.jsx)(n.td,{children:"Server health, uptime, and service status"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"GET"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/formats"})}),(0,i.jsx)(n.td,{children:"Supported audio formats and optimal settings"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"POST"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"/upload"})}),(0,i.jsx)(n.td,{children:"Audio upload and transcription"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"component-architecture",children:"Component Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"1-client-layer-game-application",children:"1. Client Layer (Game Application)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Technology"}),": JavaScript (Browser, Node.js, or React Native)"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Responsibilities"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Capture audio input from AAC devices or microphone"}),"\n",(0,i.jsx)(n.li,{children:"Convert audio to WAV format (16kHz, 16-bit, mono recommended)"}),"\n",(0,i.jsx)(n.li,{children:"Send HTTP requests to API endpoints"}),"\n",(0,i.jsx)(n.li,{children:"Parse JSON responses (camelCase format)"}),"\n",(0,i.jsx)(n.li,{children:"Map transcribed text to game commands"}),"\n",(0,i.jsx)(n.li,{children:"Handle errors and provide user feedback"}),"\n",(0,i.jsx)(n.li,{children:"Display confidence scores and processing times"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key Interfaces"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Health Check Request\nGET /health\nResponse: {\n  status: \"ok\",\n  timestamp: string,\n  uptime: number,\n  uptimeFormatted: string,\n  version: string,\n  services: { speechRecognition: boolean, logging: boolean },\n  supportedFormats: string[],\n  endpoints: object\n}\n\n// Upload Request\nPOST /upload\nContent-Type: multipart/form-data\nBody: { audioFile: File }\nHeaders: {\n  'x-command-mode'?: 'true' | 'false',\n  'x-user-id'?: string,\n  'x-session-id'?: string,\n  'x-logging-consent'?: 'true' | 'false'\n}\n\n// Success Response (camelCase)\n{\n  success: true,\n  transcription: string,\n  confidence: number,          // 0-1\n  service: string,             // \"google\" | \"vosk\"\n  processingTimeMs: number,\n  audio: {\n    filename: string,\n    size: number,\n    sizeBytes: number,\n    format: string,\n    duration: number,\n    sampleRate: number,\n    channels: number,\n    mimeType: string\n  },\n  request: {\n    timestamp: string,\n    device: string,\n    browser: string,\n    userAgent: string\n  },\n  aac: {\n    commandMode: boolean,\n    commandType: string | null,\n    isCommand: boolean,\n    suggestedActions?: string[]\n  },\n  wordTiming?: Array<{\n    word: string,\n    startTime: number,\n    endTime: number,\n    confidence: number\n  }>,\n  user?: { id: string },\n  warnings?: string[]\n}\n\n// Error Response (camelCase)\n{\n  success: false,\n  transcription: null,\n  processingTimeMs: number,\n  error: {\n    code: string,\n    message: string,\n    details?: Array<{ service: string, error: string }>\n  },\n  request: { ... },\n  audio?: { ... },\n  warnings?: string[]\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-server-layer-expressjs",children:"2. Server Layer (Express.js)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Technology"}),": Node.js with Express framework"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"File"}),": ",(0,i.jsx)(n.code,{children:"index.js"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Responsibilities"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Handle HTTP requests and routing"}),"\n",(0,i.jsx)(n.li,{children:"Serve health and format information endpoints"}),"\n",(0,i.jsx)(n.li,{children:"Process multipart/form-data uploads"}),"\n",(0,i.jsx)(n.li,{children:"Validate file presence and size"}),"\n",(0,i.jsx)(n.li,{children:"Spawn and manage Python subprocess"}),"\n",(0,i.jsx)(n.li,{children:"Pass command mode flags to Python"}),"\n",(0,i.jsx)(n.li,{children:"Parse user agent and device information"}),"\n",(0,i.jsx)(n.li,{children:"Implement consent-based logging"}),"\n",(0,i.jsx)(n.li,{children:"Build standardized camelCase JSON responses"}),"\n",(0,i.jsx)(n.li,{children:"Handle 404 routes with helpful error messages"}),"\n",(0,i.jsx)(n.li,{children:"Global error handling for unexpected exceptions"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key Modules"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Core Modules\nconst express = require('express');\nconst multer = require('multer');\nconst cors = require('cors');\nconst fs = require('fs');\nconst { spawn } = require('child_process');\nconst path = require('path');\n\n// Configuration\nconst PORT = process.env.PORT || 8080;\nconst storage = multer.memoryStorage();\nconst upload = multer({ \n  storage,\n  limits: { fileSize: 10 * 1024 * 1024 } // 10MB limit\n});\nconst LOG_DIR = path.join(__dirname, 'logs');\nconst SPEECH_SCRIPT = path.join(__dirname, 'speechRecognition.py');\nconst SERVER_START_TIME = Date.now();\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Helper Functions"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"parseUserAgent(userAgent)"})," - Extracts browser and device type"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"detectAudioFormat(filename)"})," - Detects format from file extension"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"logRequest(data, consentGiven)"})," - Writes request logs to JSON files"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"buildSuccessResponse(params)"})," - Constructs standardized success response"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"buildErrorResponse(params)"})," - Constructs standardized error response"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"parsePythonOutput(output)"})," - Parses Python JSON with backwards compatibility"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Route Handlers"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Health check endpoint\napp.get('/health', (req, res) => {\n  // Returns server status, uptime, services, supported formats\n});\n\n// Supported formats endpoint\napp.get('/formats', (req, res) => {\n  // Returns format list and optimal settings\n});\n\n// Audio upload and transcription\napp.post('/upload', upload.single(\"audioFile\"), async (req, res) => {\n  // 1. Validate file presence\n  // 2. Check command mode header\n  // 3. Extract metadata\n  // 4. Spawn Python process with flags\n  // 5. Pipe audio buffer to Python stdin\n  // 6. Collect stdout/stderr\n  // 7. Parse JSON response\n  // 8. Build standardized response\n  // 9. Log with consent\n  // 10. Return response\n});\n\n// 404 handler\napp.use((req, res) => {\n  // Returns available endpoints\n});\n\n// Global error handler\napp.use((err, req, res, next) => {\n  // Handles multer errors, unexpected exceptions\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-audio-processing-layer-python",children:"3. Audio Processing Layer (Python)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Technology"}),": Python 3.8+ with SpeechRecognition, Vosk, NumPy, SciPy"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"File"}),": ",(0,i.jsx)(n.code,{children:"speechRecognition.py"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Responsibilities"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Read audio data from stdin"}),"\n",(0,i.jsx)(n.li,{children:"Detect audio format from file headers"}),"\n",(0,i.jsx)(n.li,{children:"Extract audio metadata (duration, sample rate, channels)"}),"\n",(0,i.jsx)(n.li,{children:"Validate audio quality (duration, volume, sample rate)"}),"\n",(0,i.jsx)(n.li,{children:"Apply audio preprocessing (high-pass filter for noise reduction)"}),"\n",(0,i.jsx)(n.li,{children:"Perform speech-to-text with fallback chain"}),"\n",(0,i.jsx)(n.li,{children:"Support AAC command mode with limited vocabulary"}),"\n",(0,i.jsx)(n.li,{children:"Extract word-level timing information"}),"\n",(0,i.jsx)(n.li,{children:"Classify commands into AAC categories"}),"\n",(0,i.jsx)(n.li,{children:"Return standardized camelCase JSON response"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Module Structure"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Core Classes/Functions\nclass AACVoiceClient:       # Low-level API communication\nclass AACCommandMapper:     # Maps phrases to actions  \nclass AACGameController:    # High-level game integration\n\n# Audio Processing\ndef detect_audio_format(audio_bytes) -> str\ndef get_wav_metadata(audio_file) -> dict\ndef preprocess_audio(recognizer, audio) -> AudioData\ndef validate_audio_quality(audio, sample_rate, duration) -> dict\n\n# Speech Recognition\ndef load_vosk_model(model_path) -> Model\ndef recognize_vosk(audio_data, model, command_mode) -> tuple\ndef recognize_with_fallback(recognizer, audio, metadata, command_mode) -> dict\n\n# AAC Features\ndef classify_command(text) -> str\ndef extract_word_timing(vosk_result) -> list\ndef get_suggested_actions(text, command_type) -> list\n\n# Response Builders\ndef build_success_response(...) -> dict\ndef build_error_response(...) -> dict\n\n# Entry Points\ndef process_audio(audio_bytes, command_mode, skip_preprocessing) -> dict\ndef main()  # CLI entry point\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Recognition Fallback Chain"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def recognize_with_fallback(recognizer, audio, metadata, command_mode):\n    errors = []\n    \n    # 1. Try Google Speech Recognition (primary, more accurate)\n    if not command_mode:  # Skip for command mode - Vosk is faster\n        try:\n            text = recognizer.recognize_google(audio)\n            return build_success_response(text, "google", ...)\n        except Exception as e:\n            errors.append({"service": "google", "error": str(e)})\n    \n    # 2. Try Vosk offline recognition (fallback, works offline)\n    try:\n        text, confidence, result = recognize_vosk(audio, model, command_mode)\n        if text:\n            return build_success_response(text, "vosk", confidence, ...)\n    except Exception as e:\n        errors.append({"service": "vosk", "error": str(e)})\n    \n    # 3. All services failed\n    return build_error_response("ALL_SERVICES_FAILED", errors, ...)\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"AAC Command Categories"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'COMMAND_CATEGORIES = {\n    "navigation": ["back", "next", "previous", "home", "menu", "exit", \n                   "up", "down", "left", "right"],\n    "selection":  ["select", "choose", "pick", "open", "close", \n                   "cancel", "confirm", "delete", "yes", "no"],\n    "communication": ["hello", "goodbye", "thank you", "please", \n                      "sorry", "wait", "more", "done", "help"],\n    "media": ["play", "pause", "stop", "repeat", "louder", "quieter"]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Audio Preprocessing Pipeline"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def preprocess_audio(recognizer, audio):\n    # 1. Convert to numpy array\n    raw_data = np.frombuffer(audio.frame_data, np.int16)\n    \n    # 2. Apply high-pass filter (remove < 80Hz noise)\n    nyquist = audio.sample_rate / 2\n    normal_cutoff = 80 / nyquist\n    b, a = signal.butter(4, normal_cutoff, btype='high')\n    filtered_data = signal.filtfilt(b, a, raw_data)\n    \n    # 3. Normalize to prevent clipping\n    max_val = np.max(np.abs(filtered_data))\n    if max_val > 0:\n        filtered_data = filtered_data * (32767 * 0.9 / max_val)\n    \n    # 4. Return processed AudioData\n    return sr.AudioData(filtered_data.astype(np.int16).tobytes(), ...)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"4-external-service-layer",children:"4. External Service Layer"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Primary Service"}),": Google Speech Recognition API"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Accessed through Python's ",(0,i.jsx)(n.code,{children:"speech_recognition"})," library"]}),"\n",(0,i.jsx)(n.li,{children:"Higher accuracy for general speech"}),"\n",(0,i.jsx)(n.li,{children:"Requires internet connection"}),"\n",(0,i.jsx)(n.li,{children:"No API key needed for basic usage"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Fallback Service"}),": Vosk Offline Recognition"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Local speech recognition using pre-trained models"}),"\n",(0,i.jsx)(n.li,{children:"Works without internet connection"}),"\n",(0,i.jsx)(n.li,{children:"Faster for short commands"}),"\n",(0,i.jsx)(n.li,{children:"Provides word-level timing and confidence"}),"\n",(0,i.jsx)(n.li,{children:"Supports custom vocabulary/grammar"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Model Configuration"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Vosk model path (configurable via environment)\nvosk_model_path = os.environ.get(\n    'VOSK_MODEL_PATH', \n    'model/vosk-model-small-en-us-0.15'\n)\n\n# Model caching for performance\nVOSK_MODEL = None  # Global cache\n\ndef load_vosk_model(model_path):\n    global VOSK_MODEL\n    if VOSK_MODEL is not None:\n        return VOSK_MODEL  # Return cached model\n    # ... load model\n    VOSK_MODEL = vosk.Model(model_path)\n    return VOSK_MODEL\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"data-flow-diagrams",children:"Data Flow Diagrams"}),"\n",(0,i.jsx)(n.h3,{id:"successful-request-flow",children:"Successful Request Flow"}),"\n",(0,i.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant Client as Game Client\n    participant Express as Express Server\n    participant Multer as Multer Middleware\n    participant Python as Python Module\n    participant Google as Google Speech API\n    participant Vosk as Vosk (Offline)\n    participant Logs as Log Files\n    \n    Client->>Express: POST /upload (audio + headers)\n    Express->>Multer: Process multipart data\n    Multer->>Express: Return file buffer\n    Express->>Express: Extract metadata<br/>(user-agent, command-mode)\n    Express->>Python: spawn() + args + pipe audio\n    \n    activate Python\n    Python->>Python: Detect format & validate\n    Python->>Python: Preprocess audio<br/>(noise filter)\n    \n    alt Command Mode Disabled\n        Python->>Google: recognize_google(audio)\n        alt Google Success\n            Google--\x3e>Python: Transcribed text\n        else Google Fails\n            Python->>Vosk: recognize(audio)\n            Vosk--\x3e>Python: Text + timing + confidence\n        end\n    else Command Mode Enabled\n        Python->>Vosk: recognize(audio, grammar)\n        Vosk--\x3e>Python: Text + timing + confidence\n    end\n    \n    Python->>Python: Classify command type\n    Python->>Python: Build camelCase response\n    Python--\x3e>Express: stdout: JSON data\n    deactivate Python\n    \n    Express->>Express: Parse Python output\n    Express->>Express: Build response object\n    \n    alt Consent Given\n        Express->>Logs: Write request log\n    end\n    \n    Express--\x3e>Client: 200 OK + JSON response\n    Client->>Client: Check confidence threshold\n    Client->>Client: Map to game command\n    Client->>Client: Execute action"}),"\n",(0,i.jsx)(n.h3,{id:"error-handling-flow",children:"Error Handling Flow"}),"\n",(0,i.jsx)(n.mermaid,{value:'sequenceDiagram\n    participant Client as Game Client\n    participant Express as Express Server\n    participant Python as Python Module\n    participant Services as Recognition Services\n    \n    Client->>Express: POST /upload\n    \n    alt No File Attached\n        Express--\x3e>Client: 400 { error: { code: "NO_FILE" } }\n    \n    else File Too Large\n        Express--\x3e>Client: 413 { error: { code: "FILE_TOO_LARGE" } }\n    \n    else Audio Quality Issues\n        Express->>Python: spawn() + pipe audio\n        Python->>Python: validate_audio_quality()\n        Python--\x3e>Express: { success: false, error: { code: "AUDIO_QUALITY_ISSUES" } }\n        Express--\x3e>Client: 422 { error: { code: "AUDIO_QUALITY_ISSUES" } }\n    \n    else All Services Fail\n        Express->>Python: spawn() + pipe audio\n        Python->>Services: Try Google \u2192 Try Vosk\n        Services--\x3e>Python: All fail\n        Python--\x3e>Express: { success: false, error: { code: "ALL_SERVICES_FAILED" } }\n        Express--\x3e>Client: 422 { error: { code: "ALL_SERVICES_FAILED", details: [...] } }\n    \n    else Server Error\n        Express->>Express: Exception thrown\n        Express--\x3e>Client: 500 { error: { code: "INTERNAL_ERROR" } }\n    end'}),"\n",(0,i.jsx)(n.h3,{id:"complete-use-case-aac-user-playing-tictactoe",children:"Complete Use Case: AAC User Playing TicTacToe"}),"\n",(0,i.jsx)(n.mermaid,{value:'sequenceDiagram\n    participant User as AAC User\n    participant AAC as AAC Device\n    participant Game as TicTacToe Client\n    participant API as Express API\n    participant Python as Python Module\n    participant Vosk as Vosk Recognition\n    \n    User->>AAC: Tap "Center" on AAC board\n    AAC->>AAC: Generate speech: "center"\n    AAC->>Game: Audio captured by browser\n    \n    Game->>Game: Start recording (3 sec)\n    Game->>Game: Convert to WAV\n    Game->>API: POST /upload<br/>Headers: x-command-mode: true\n    \n    activate API\n    API->>API: Validate file (OK)\n    API->>API: Extract metadata\n    API->>Python: spawn --command-mode<br/>+ pipe audio\n    \n    activate Python\n    Python->>Python: Validate audio quality (OK)\n    Python->>Python: Preprocess (filter noise)\n    Python->>Vosk: recognize with AAC grammar\n    Vosk--\x3e>Python: "center" (conf: 0.94)\n    Python->>Python: classify_command("center")<br/>\u2192 "selection"\n    Python->>Python: Build response\n    Python--\x3e>API: JSON response\n    deactivate Python\n    \n    API->>API: Parse & build response\n    API--\x3e>Game: 200 OK\n    deactivate API\n    \n    Note over Game: Response received\n    Game->>Game: Check confidence (0.94 > 0.4) \u2713\n    Game->>Game: Match "center" \u2192 position 4\n    Game->>Game: handleClick(4)\n    Game->>User: Display X in center\n    Game->>User: Speak "X placed in center"'}),"\n",(0,i.jsx)(n.h3,{id:"health-check-flow",children:"Health Check Flow"}),"\n",(0,i.jsx)(n.mermaid,{value:'sequenceDiagram\n    participant Client as Game Client\n    participant API as Express Server\n    participant FS as File System\n    \n    Client->>API: GET /health\n    \n    API->>API: Calculate uptime\n    API->>FS: Check script exists\n    FS--\x3e>API: true\n    API->>FS: Check log dir exists\n    FS--\x3e>API: true\n    \n    API--\x3e>Client: 200 OK\n    Note over Client: { status: "ok",<br/>version: "2.0.0",<br/>services: { speechRecognition: true } }\n    \n    Client->>Client: Update UI status indicator'}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"class-diagrams",children:"Class Diagrams"}),"\n",(0,i.jsx)(n.h3,{id:"express-server-architecture",children:"Express Server Architecture"}),"\n",(0,i.jsx)(n.mermaid,{value:"classDiagram\n    class ExpressApp {\n        +express app\n        +number PORT\n        +multer upload\n        +string LOG_DIR\n        +string SPEECH_SCRIPT\n        +number SERVER_START_TIME\n        +listen()\n        +use(middleware)\n    }\n    \n    class HealthHandler {\n        +handleHealth(req, res)\n        -calculateUptime()\n        -checkServices()\n        -getSupportedFormats()\n    }\n    \n    class FormatsHandler {\n        +handleFormats(req, res)\n        -getOptimalSettings()\n    }\n    \n    class UploadHandler {\n        +handleUpload(req, res)\n        -validateFile(req)\n        -checkCommandMode(req)\n        -extractMetadata(req)\n        -spawnPythonProcess(buffer, flags)\n        -parsePythonOutput(stdout)\n        -buildResponse(data)\n    }\n    \n    class ResponseBuilder {\n        +buildSuccessResponse(params)\n        +buildErrorResponse(params)\n        -formatAudioMeta(file, pythonResult)\n        -formatRequestMeta(req)\n    }\n    \n    class MetadataParser {\n        +parseUserAgent(userAgent)\n        +detectAudioFormat(filename)\n        -detectBrowser(ua)\n        -detectDevice(ua)\n    }\n    \n    class LoggingService {\n        +logRequest(data, consent)\n        -checkConsent(req)\n        -getLogFile(date)\n        -appendLog(file, entry)\n    }\n    \n    class PythonInterface {\n        +spawn(scriptPath, args)\n        +writeToStdin(buffer)\n        +collectOutput()\n        +handleExit(code)\n    }\n    \n    class ErrorHandler {\n        +handleNotFound(req, res)\n        +handleGlobalError(err, req, res)\n        -formatErrorResponse(code, message)\n    }\n    \n    ExpressApp --\x3e HealthHandler\n    ExpressApp --\x3e FormatsHandler\n    ExpressApp --\x3e UploadHandler\n    ExpressApp --\x3e ErrorHandler\n    UploadHandler --\x3e ResponseBuilder\n    UploadHandler --\x3e MetadataParser\n    UploadHandler --\x3e LoggingService\n    UploadHandler --\x3e PythonInterface"}),"\n",(0,i.jsx)(n.h3,{id:"python-module-architecture",children:"Python Module Architecture"}),"\n",(0,i.jsx)(n.mermaid,{value:"classDiagram\n    class SpeechRecognitionModule {\n        +process_audio(bytes, command_mode)\n        +main()\n        -warm_up_models()\n        -get_model_status()\n    }\n    \n    class AudioProcessor {\n        +detect_audio_format(bytes)\n        +get_wav_metadata(file)\n        +preprocess_audio(recognizer, audio)\n        +validate_audio_quality(audio, rate, duration)\n        -apply_highpass_filter(data, rate)\n        -normalize_audio(data)\n    }\n    \n    class RecognitionService {\n        +recognize_with_fallback(recognizer, audio, meta, cmd_mode)\n        +recognize_vosk(audio, model, cmd_mode)\n        -load_vosk_model(path)\n        -try_google(recognizer, audio)\n        -try_vosk(audio, model)\n    }\n    \n    class AACFeatures {\n        +classify_command(text)\n        +extract_word_timing(result)\n        +get_suggested_actions(text, type)\n        -COMMAND_CATEGORIES\n        -AAC_COMMANDS\n    }\n    \n    class ResponseBuilder {\n        +build_success_response(...)\n        +build_error_response(...)\n        -format_audio_meta(metadata)\n        -format_aac_info(text, cmd_mode)\n    }\n    \n    class ConfigManager {\n        +vosk_model_path\n        +VOSK_MODEL\n        +AAC_COMMANDS\n        +COMMAND_CATEGORIES\n        +SUPPORTED_FORMATS\n    }\n    \n    SpeechRecognitionModule --\x3e AudioProcessor\n    SpeechRecognitionModule --\x3e RecognitionService\n    SpeechRecognitionModule --\x3e ResponseBuilder\n    RecognitionService --\x3e AACFeatures\n    RecognitionService --\x3e ConfigManager\n    ResponseBuilder --\x3e AACFeatures"}),"\n",(0,i.jsx)(n.h3,{id:"client-integration-module",children:"Client Integration Module"}),"\n",(0,i.jsx)(n.mermaid,{value:"classDiagram\n    class AACGameController {\n        +AACVoiceClient client\n        +AACCommandMapper mapper\n        +object options\n        +boolean isActive\n        +HTMLElement ui\n        +mapCommand(phrases, action, options)\n        +mapCommonCommands(actions)\n        +unmapCommand(phrase)\n        +clearCommands()\n        +start(continuous)\n        +stop()\n        +toggle()\n        +checkHealth()\n        +getCommands()\n        +showUI()\n        +hideUI()\n        +destroy()\n    }\n    \n    class AACVoiceClient {\n        +string apiUrl\n        +boolean commandMode\n        +boolean isListening\n        +boolean isRecording\n        +MediaStream stream\n        +checkHealth()\n        +getFormats()\n        +startListening(continuous)\n        +stopListening()\n        -_startContinuousRecording()\n        -_startSingleRecording()\n        -_sendToAPI(blob)\n        -_convertToWav(blob)\n    }\n    \n    class AACCommandMapper {\n        +Map commands\n        +number defaultConfidenceThreshold\n        +register(phrases, action, options)\n        +unregister(phrase)\n        +clear()\n        +process(result)\n        +getCommandsByCategory()\n        +getAllPhrases()\n    }\n    \n    class CommandMapping {\n        +Function action\n        +number confidence\n        +boolean exactMatch\n        +string category\n        +string[] phrases\n    }\n    \n    AACGameController --\x3e AACVoiceClient\n    AACGameController --\x3e AACCommandMapper\n    AACCommandMapper --\x3e CommandMapping"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"database-design",children:"Database Design"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Current Status"}),": The API uses ",(0,i.jsx)(n.strong,{children:"file-based logging"})," for request tracking and analytics. No traditional database is required."]}),"\n",(0,i.jsx)(n.h3,{id:"log-file-structure",children:"Log File Structure"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Location"}),": ",(0,i.jsx)(n.code,{children:"Initial_API/logs/"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Format"}),": JSON array per daily log file"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Filename Pattern"}),": ",(0,i.jsx)(n.code,{children:"requests-YYYY-MM-DD.json"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Schema (v2.0.0)"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'[\n  {\n    "timestamp": "2025-01-15T14:23:45.123Z",\n    "success": true,\n    "transcription": "center",\n    "confidence": 0.94,\n    "service": "vosk",\n    "processingTimeMs": 245,\n    "audio": {\n      "filename": "recording.wav",\n      "size": 32000,\n      "sizeBytes": 32000,\n      "format": "WAV",\n      "duration": 1.5,\n      "sampleRate": 16000,\n      "channels": 1,\n      "mimeType": "audio/wav"\n    },\n    "request": {\n      "timestamp": "2025-01-15T14:23:43.000Z",\n      "device": "Desktop",\n      "browser": "Chrome",\n      "userAgent": "Mozilla/5.0..."\n    },\n    "aac": {\n      "commandMode": true,\n      "commandType": "selection",\n      "isCommand": true\n    },\n    "user": {\n      "id": "user123"\n    },\n    "audioBufferSize": 32000,\n    "ipAddress": "::1"\n  }\n]\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"algorithms-and-processing-logic",children:"Algorithms and Processing Logic"}),"\n",(0,i.jsx)(n.h3,{id:"1-audio-format-detection-algorithm",children:"1. Audio Format Detection Algorithm"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Location"}),": ",(0,i.jsx)(n.code,{children:"speechRecognition.py"})," - ",(0,i.jsx)(n.code,{children:"detect_audio_format()"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Identify audio format from magic bytes (file signature)"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def detect_audio_format(audio_bytes: bytes) -> str:\n    \"\"\"Detect audio format from byte stream.\"\"\"\n    if len(audio_bytes) < 12:\n        return 'UNKNOWN'\n    \n    # Check magic bytes\n    if audio_bytes.startswith(b'RIFF') and audio_bytes[8:12] == b'WAVE':\n        return 'WAV'\n    elif audio_bytes[0:3] == b'ID3' or audio_bytes[0:2] == b'\\xff\\xfb':\n        return 'MP3'\n    elif audio_bytes[0:4] == b'fLaC':\n        return 'FLAC'\n    elif audio_bytes[0:4] == b'OggS':\n        return 'OGG'\n    else:\n        return 'UNKNOWN'\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Magic Bytes Reference"}),":"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Format"}),(0,i.jsx)(n.th,{children:"Magic Bytes"}),(0,i.jsx)(n.th,{children:"Hex"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"WAV"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"RIFF....WAVE"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"52 49 46 46 xx xx xx xx 57 41 56 45"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"MP3"}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"ID3"})," or ",(0,i.jsx)(n.code,{children:"\xff\xfb"})]}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"49 44 33"})," or ",(0,i.jsx)(n.code,{children:"FF FB"})]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"FLAC"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"fLaC"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"66 4C 61 43"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"OGG"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"OggS"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"4F 67 67 53"})})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"2-audio-quality-validation-algorithm",children:"2. Audio Quality Validation Algorithm"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Location"}),": ",(0,i.jsx)(n.code,{children:"speechRecognition.py"})," - ",(0,i.jsx)(n.code,{children:"validate_audio_quality()"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Ensure audio meets minimum quality requirements for recognition"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def validate_audio_quality(audio, sample_rate, duration):\n    issues = []\n    warnings = []\n    \n    # Duration checks\n    if duration < 0.1:\n        issues.append("Audio too short (< 0.1s)")\n    elif duration < 0.3:\n        warnings.append("Audio may be too short")\n    elif duration > 30:\n        warnings.append("Long audio may increase processing time")\n    \n    # Volume check (RMS)\n    rms = audioop.rms(audio.frame_data, audio.sample_width)\n    if rms < 50:\n        issues.append("Audio appears silent")\n    elif rms < 200:\n        warnings.append("Audio volume is low")\n    \n    # Sample rate check\n    if sample_rate < 8000:\n        issues.append("Sample rate too low (< 8kHz)")\n    elif sample_rate < 16000:\n        warnings.append("Sample rate below optimal (16kHz)")\n    \n    return {\n        "valid": len(issues) == 0,\n        "issues": issues,\n        "warnings": warnings\n    }\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-audio-preprocessing-algorithm",children:"3. Audio Preprocessing Algorithm"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Location"}),": ",(0,i.jsx)(n.code,{children:"speechRecognition.py"})," - ",(0,i.jsx)(n.code,{children:"preprocess_audio()"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Remove noise and normalize audio for better recognition"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def preprocess_audio(recognizer, audio):\n    # Convert to numpy array\n    raw_data = np.frombuffer(audio.frame_data, np.int16)\n    \n    # Design high-pass Butterworth filter\n    nyquist = audio.sample_rate / 2\n    cutoff = 80  # Remove frequencies below 80Hz\n    normal_cutoff = cutoff / nyquist\n    b, a = signal.butter(4, normal_cutoff, btype='high')\n    \n    # Apply zero-phase filtering\n    filtered_data = signal.filtfilt(b, a, raw_data)\n    \n    # Normalize to prevent clipping\n    max_val = np.max(np.abs(filtered_data))\n    if max_val > 0:\n        filtered_data = filtered_data * (32767 * 0.9 / max_val)\n    \n    # Clip to valid range\n    filtered_data = np.clip(filtered_data, -32768, 32767)\n    \n    return sr.AudioData(\n        filtered_data.astype(np.int16).tobytes(),\n        audio.sample_rate,\n        audio.sample_width\n    )\n"})}),"\n",(0,i.jsx)(n.h3,{id:"4-command-classification-algorithm",children:"4. Command Classification Algorithm"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Location"}),": ",(0,i.jsx)(n.code,{children:"speechRecognition.py"})," - ",(0,i.jsx)(n.code,{children:"classify_command()"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Categorize recognized text into AAC command types"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def classify_command(text: str) -> Optional[str]:\n    if not text:\n        return None\n    \n    text_lower = text.lower().strip()\n    words = text_lower.split()\n    \n    # Check each word against command categories\n    for category, commands in COMMAND_CATEGORIES.items():\n        for word in words:\n            if word in commands:\n                return category\n    \n    return "freeform"  # Not a recognized command\n'})}),"\n",(0,i.jsx)(n.h3,{id:"5-user-agent-parsing-algorithm",children:"5. User Agent Parsing Algorithm"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Location"}),": ",(0,i.jsx)(n.code,{children:"index.js"})," - ",(0,i.jsx)(n.code,{children:"parseUserAgent()"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Extract browser and device information from user-agent string"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"function parseUserAgent(userAgent) {\n    if (!userAgent) return { browser: 'Unknown', device: 'Unknown' };\n    \n    const ua = userAgent.toLowerCase();\n    let browser = 'Unknown';\n    let device = 'Unknown';\n    \n    // Browser detection (order matters for accuracy)\n    if (ua.includes('edg')) browser = 'Edge';\n    else if (ua.includes('chrome') && !ua.includes('edg')) browser = 'Chrome';\n    else if (ua.includes('firefox')) browser = 'Firefox';\n    else if (ua.includes('safari') && !ua.includes('chrome')) browser = 'Safari';\n    else if (ua.includes('opera')) browser = 'Opera';\n    \n    // Device detection\n    if (ua.includes('mobile') || ua.includes('android') || ua.includes('iphone')) {\n        device = 'Mobile';\n    } else if (ua.includes('tablet') || ua.includes('ipad')) {\n        device = 'Tablet';\n    } else {\n        device = 'Desktop';\n    }\n    \n    return { browser, device };\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"6-confidence-based-command-matching",children:"6. Confidence-Based Command Matching"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Location"}),": ",(0,i.jsx)(n.code,{children:"aac-voice-control.js"})," - ",(0,i.jsx)(n.code,{children:"AACCommandMapper.process()"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Match transcription to commands with confidence filtering"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"process(result) {\n    const text = result.text.toLowerCase().trim();\n    const confidence = result.confidence || 0;\n    \n    // Try exact match first\n    if (this.commands.has(text)) {\n        const cmd = this.commands.get(text);\n        if (confidence >= cmd.confidence) {\n            cmd.action(result);\n            return { matched: true, phrase: text, type: 'exact' };\n        }\n    }\n    \n    // Try partial match\n    for (const [phrase, cmd] of this.commands) {\n        if (cmd.exactMatch) continue;\n        \n        if (text.includes(phrase) && confidence >= cmd.confidence) {\n            cmd.action(result);\n            return { matched: true, phrase, type: 'partial' };\n        }\n    }\n    \n    return { matched: false, text };\n}\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"1-input-validation",children:"1. Input Validation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"File presence check"}),": Returns 400 error if no file uploaded"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"File size limit"}),": 10MB maximum (configurable via Multer)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Format validation"}),": Python validates audio headers before processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio quality validation"}),": Rejects silent or corrupted audio"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-data-privacy",children:"2. Data Privacy"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Consent-based logging"}),": No data stored without explicit permission"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Production mode"}),": Auto-consent disabled in production environment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IP handling"}),": Consider hashing IPs before logging in production"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GDPR compliance"}),": User data deletable by removing log files"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"No persistent storage"}),": Audio not saved to disk (memory only)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-api-security",children:"3. API Security"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CORS enabled"}),": Configure allowed origins for production"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"No authentication"}),": Open API (add API keys/OAuth for production)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Rate limiting"}),": Implement for production deployments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"File type restrictions"}),": Only audio MIME types accepted"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"4-subprocess-security",children:"4. Subprocess Security"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fixed script path"}),": No user-controlled script execution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Stdin-only input"}),": No file system access from user data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Process isolation"}),": Python runs in separate process"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Timeout handling"}),": Processes killed after timeout"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"5-recommended-production-hardening",children:"5. Recommended Production Hardening"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Add to production deployment\nconst rateLimit = require('express-rate-limit');\nconst helmet = require('helmet');\n\napp.use(helmet());\napp.use(rateLimit({\n    windowMs: 15 * 60 * 1000, // 15 minutes\n    max: 100 // limit each IP to 100 requests per window\n}));\n\n// Restrict CORS\napp.use(cors({\n    origin: ['https://yourdomain.com'],\n    methods: ['GET', 'POST']\n}));\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"1-memory-usage",children:"1. Memory Usage"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"In-memory buffers"}),": Multer stores files in RAM (fast, limited by server memory)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Streaming"}),": Audio piped to Python via stdin (no disk I/O)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model caching"}),": Vosk model loaded once, reused across requests"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Garbage collection"}),": Buffers released after request completion"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-concurrency",children:"2. Concurrency"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Async I/O"}),": Node.js event loop handles concurrent requests"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Subprocess per request"}),": Each request spawns new Python process"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Model preloading"}),": Vosk model warmed up on server start"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Non-blocking"}),": Server responsive during Python processing"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-response-times",children:"3. Response Times"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Scenario"}),(0,i.jsx)(n.th,{children:"Typical Time"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Health check"}),(0,i.jsx)(n.td,{children:"< 10ms"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Short command (1-2s audio, Vosk)"}),(0,i.jsx)(n.td,{children:"200-500ms"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Short command (1-2s audio, Google)"}),(0,i.jsx)(n.td,{children:"500-1500ms"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Long audio (10-20s, Google)"}),(0,i.jsx)(n.td,{children:"2-5s"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Model cold start (first request)"}),(0,i.jsx)(n.td,{children:"+2-3s"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"4-optimization-strategies",children:"4. Optimization Strategies"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Enable command mode"})," for AAC applications (faster Vosk recognition)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Preload Vosk model"})," on server startup (",(0,i.jsx)(n.code,{children:"PRELOAD_VOSK=true"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use WAV format"})," (no transcoding needed)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Optimal audio settings"}),": 16kHz, 16-bit, mono"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Client-side VAD"}),": Only send audio with speech detected"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"deployment-architecture",children:"Deployment Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"development-environment",children:"Development Environment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       Developer Machine             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u2022 Node.js + Express (localhost:8080)\u2502\n\u2502 \u2022 Python 3.8+ with dependencies     \u2502\n\u2502 \u2022 Vosk model (optional)             \u2502\n\u2502 \u2022 Test audio files                  \u2502\n\u2502 \u2022 Git repository                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h3,{id:"production-environment-recommended",children:"Production Environment (Recommended)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Internet                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Load Balancer (nginx/ALB)                  \u2502\n\u2502              \u2022 SSL termination                          \u2502\n\u2502              \u2022 Rate limiting                            \u2502\n\u2502              \u2022 Health checks                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u25bc               \u25bc               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   App 1     \u2502  \u2502   App 2     \u2502  \u2502   App 3     \u2502\n\u2502 (Express +  \u2502  \u2502 (Express +  \u2502  \u2502 (Express +  \u2502\n\u2502  Python)    \u2502  \u2502  Python)    \u2502  \u2502  Python)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                \u2502                \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2502\n                        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Shared Storage (S3 / EFS / NFS)               \u2502\n\u2502           \u2022 Vosk model files                            \u2502\n\u2502           \u2022 Log files (optional)                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h3,{id:"docker-deployment",children:"Docker Deployment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-dockerfile",children:'# Dockerfile\nFROM node:18-slim\n\n# Install Python and dependencies\nRUN apt-get update && apt-get install -y \\\n    python3 python3-pip \\\n    && rm -rf /var/lib/apt/lists/*\n\nWORKDIR /app\n\n# Install Node dependencies\nCOPY package*.json ./\nRUN npm ci --only=production\n\n# Install Python dependencies\nRUN pip3 install --break-system-packages \\\n    SpeechRecognition vosk numpy scipy\n\n# Copy application\nCOPY . .\n\n# Download Vosk model (optional, or mount as volume)\n# RUN mkdir -p model && wget -O model.zip ... && unzip ...\n\nEXPOSE 8080\n\nCMD ["node", "index.js"]\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsxs)(n.p,{children:["The AAC Integration API v2.0.0 implements a ",(0,i.jsx)(n.strong,{children:"three-tier architecture"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Client Layer"}),": Game applications that capture audio, send HTTP requests, and execute commands based on transcription"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Server Layer"}),": Express.js API that handles uploads, manages subprocesses, and returns standardized responses"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Processing Layer"}),": Python module with multi-service recognition, audio preprocessing, and AAC-specific features"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"key-architectural-decisions",children:"Key Architectural Decisions"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Decision"}),(0,i.jsx)(n.th,{children:"Rationale"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"RESTful API"}),(0,i.jsx)(n.td,{children:"Simplicity, broad client compatibility"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Subprocess architecture"}),(0,i.jsx)(n.td,{children:"Language flexibility (Node.js + Python)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Multi-service fallback"}),(0,i.jsx)(n.td,{children:"Reliability (Google + Vosk offline)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"camelCase JSON"}),(0,i.jsx)(n.td,{children:"JavaScript convention, consistent format"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Command mode"}),(0,i.jsx)(n.td,{children:"Optimized for AAC use case"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"File-based logging"}),(0,i.jsx)(n.td,{children:"Development simplicity, no DB setup"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Consent-based collection"}),(0,i.jsx)(n.td,{children:"Privacy compliance (GDPR)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Word-level timing"}),(0,i.jsx)(n.td,{children:"Visual feedback, accessibility features"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"version-history",children:"Version History"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Version"}),(0,i.jsx)(n.th,{children:"Changes"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"1.0.0"}),(0,i.jsx)(n.td,{children:"Initial release with Google Speech Recognition"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"2.0.0"}),(0,i.jsx)(n.td,{children:"Added Vosk fallback, command mode, word timing, camelCase responses, health endpoint, audio preprocessing"})]})]})]}),"\n",(0,i.jsx)(n.hr,{})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);