"use strict";(self.webpackChunkcreate_project_docs=self.webpackChunkcreate_project_docs||[]).push([[3791],{28453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var s=r(96540);const i={},t=s.createContext(i);function o(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(t.Provider,{value:n},e.children)}},48760:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"system-architecture/design","title":"Design Document - Part I: Architecture","description":"Purpose","source":"@site/docs/system-architecture/design.md","sourceDirName":"system-architecture","slug":"/system-architecture/design","permalink":"/project-002-aac-api/docs/system-architecture/design","draft":false,"unlisted":false,"editUrl":"https://github.com/Capstone-Projects-2025-Fall/project-002-aac-api/edit/main/documentation/docs/system-architecture/design.md","tags":[],"version":"current","lastUpdatedBy":"TheBigMo","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docsSidebar","previous":{"title":"System Architecture","permalink":"/project-002-aac-api/docs/category/system-architecture"},"next":{"title":"Development Environment","permalink":"/project-002-aac-api/docs/system-architecture/development-environment"}}');var i=r(74848),t=r(28453);const o={sidebar_position:1},a="Design Document - Part I: Architecture",l={},c=[{value:"Purpose",id:"purpose",level:2},{value:"System Architecture Overview",id:"system-architecture-overview",level:2},{value:"High-Level Architecture",id:"high-level-architecture",level:3},{value:"Component Architecture",id:"component-architecture",level:2},{value:"1. Client Layer (Game Application)",id:"1-client-layer-game-application",level:3},{value:"2. Server Layer (Express.js)",id:"2-server-layer-expressjs",level:3},{value:"3. Audio Processing Layer (Python)",id:"3-audio-processing-layer-python",level:3},{value:"4. External Service Layer",id:"4-external-service-layer",level:3},{value:"Data Flow Diagrams",id:"data-flow-diagrams",level:2},{value:"Successful Request Flow",id:"successful-request-flow",level:3},{value:"Error Handling Flow",id:"error-handling-flow",level:3},{value:"Complete Use Case: AAC User Playing Game",id:"complete-use-case-aac-user-playing-game",level:3},{value:"Class Diagrams",id:"class-diagrams",level:2},{value:"Express Server Architecture",id:"express-server-architecture",level:3},{value:"Python Script Architecture",id:"python-script-architecture",level:3},{value:"Client Integration Pattern",id:"client-integration-pattern",level:3},{value:"Database Design",id:"database-design",level:2},{value:"Log File Structure",id:"log-file-structure",level:3},{value:"Future Database Considerations",id:"future-database-considerations",level:3},{value:"Algorithms and Processing Logic",id:"algorithms-and-processing-logic",level:2},{value:"1. Audio Format Detection Algorithm",id:"1-audio-format-detection-algorithm",level:3},{value:"2. User Agent Parsing Algorithm",id:"2-user-agent-parsing-algorithm",level:3},{value:"3. Speech Recognition Pipeline",id:"3-speech-recognition-pipeline",level:3},{value:"4. Consent-Based Logging Algorithm",id:"4-consent-based-logging-algorithm",level:3},{value:"Security Considerations",id:"security-considerations",level:2},{value:"1. Input Validation",id:"1-input-validation",level:3},{value:"2. Data Privacy",id:"2-data-privacy",level:3},{value:"3. API Security",id:"3-api-security",level:3},{value:"4. Subprocess Security",id:"4-subprocess-security",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"1. Memory Usage",id:"1-memory-usage",level:3},{value:"2. Concurrency",id:"2-concurrency",level:3},{value:"3. Response Times",id:"3-response-times",level:3},{value:"Deployment Architecture",id:"deployment-architecture",level:2},{value:"Development Environment",id:"development-environment",level:3},{value:"Production Environment (Recommended)",id:"production-environment-recommended",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"design-document---part-i-architecture",children:"Design Document - Part I: Architecture"})}),"\n",(0,i.jsx)(n.h2,{id:"purpose",children:"Purpose"}),"\n",(0,i.jsx)(n.p,{children:"The Design Document - Part I Architecture describes the software architecture and how the requirements are mapped into the design. This document combines diagrams and text that describes the system's components, their interactions, and the data flow through the AAC Integration API."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"system-architecture-overview",children:"System Architecture Overview"}),"\n",(0,i.jsxs)(n.p,{children:["The AAC Integration API follows a ",(0,i.jsx)(n.strong,{children:"client-server architecture"})," with a ",(0,i.jsx)(n.strong,{children:"Python subprocess pipeline"})," for audio processing. The system is designed to be stateless, processing each audio upload request independently while maintaining optional logging capabilities for analytics."]}),"\n",(0,i.jsx)(n.h3,{id:"high-level-architecture",children:"High-Level Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Game Client   \u2502 (Browser/Desktop App)\n\u2502  (JavaScript)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 HTTP POST\n         \u2502 multipart/form-data\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Express Server \u2502 (Node.js)\n\u2502   Port: 8080    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u2022 CORS Handling \u2502\n\u2502 \u2022 File Upload   \u2502\n\u2502 \u2022 Multer        \u2502\n\u2502 \u2022 Metadata      \u2502\n\u2502 \u2022 Logging       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 spawn()\n         \u2502 stdin/stdout\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Python Script   \u2502 (speech2.py)\n\u2502   (Subprocess)  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u2022 Audio Format  \u2502\n\u2502 \u2022 Detection     \u2502\n\u2502 \u2022 Speech Recog. \u2502\n\u2502 \u2022 Google API    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 Network Call\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Google Speech  \u2502\n\u2502 Recognition API \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"component-architecture",children:"Component Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"1-client-layer-game-application",children:"1. Client Layer (Game Application)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Technology"}),": JavaScript (Browser or Node.js)"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Responsibilities"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Capture or receive audio input from AAC devices or microphone"}),"\n",(0,i.jsx)(n.li,{children:"Format audio file for upload"}),"\n",(0,i.jsx)(n.li,{children:"Send HTTP POST request to API endpoint"}),"\n",(0,i.jsx)(n.li,{children:"Parse JSON response"}),"\n",(0,i.jsx)(n.li,{children:"Map transcribed text to game commands"}),"\n",(0,i.jsx)(n.li,{children:"Handle errors and provide user feedback"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key Interfaces"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Request Interface\nPOST /upload\nContent-Type: multipart/form-data\nBody: {\n  audioFile: File,\n  userId?: string,\n  loggingConsent?: boolean\n}\nHeaders: {\n  'x-user-id'?: string,\n  'x-session-id'?: string,\n  'x-logging-consent'?: 'true' | 'false'\n}\n\n// Response Interface\n{\n  success: boolean,\n  transcription: string | null,\n  audio: AudioMetadata,\n  request: RequestMetadata,\n  user?: UserData,\n  error?: ErrorData\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-server-layer-expressjs",children:"2. Server Layer (Express.js)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Technology"}),": Node.js with Express framework"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"File"}),": ",(0,i.jsx)(n.code,{children:"index.js"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Responsibilities"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Handle HTTP requests and routing"}),"\n",(0,i.jsx)(n.li,{children:"Process multipart/form-data uploads"}),"\n",(0,i.jsx)(n.li,{children:"Spawn and manage Python subprocess"}),"\n",(0,i.jsx)(n.li,{children:"Parse user agent and device information"}),"\n",(0,i.jsx)(n.li,{children:"Implement consent-based logging"}),"\n",(0,i.jsx)(n.li,{children:"Format comprehensive JSON responses"}),"\n",(0,i.jsx)(n.li,{children:"Handle errors and exceptions"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key Classes/Modules"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"// Core Modules\nconst express = require('express');\nconst multer = require('multer');\nconst cors = require('cors');\nconst fs = require('fs');\nconst { spawn } = require('child_process');\nconst path = require('path');\n\n// Configuration\nconst PORT = 8080;\nconst storage = multer.memoryStorage();\nconst upload = multer({ storage });\nconst LOG_DIR = path.join(__dirname, 'logs');\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Helper Functions"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"parseUserAgent(userAgent)"})," - Extracts browser and device type"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"logRequest(data, consentGiven)"})," - Writes request logs to JSON files"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Main Route Handler"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"app.post('/upload', upload.single(\"audioFile\"), async (req, res) => {\n  // 1. Validate file presence\n  // 2. Extract metadata\n  // 3. Spawn Python process\n  // 4. Pipe audio buffer to Python stdin\n  // 5. Collect stdout/stderr\n  // 6. Parse JSON response\n  // 7. Log with consent\n  // 8. Return structured response\n});\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-audio-processing-layer-python",children:"3. Audio Processing Layer (Python)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Technology"}),": Python 3.x with SpeechRecognition library"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"File"}),": ",(0,i.jsx)(n.code,{children:"speech2.py"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Responsibilities"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Read audio data from stdin"}),"\n",(0,i.jsx)(n.li,{children:"Detect audio format from file headers"}),"\n",(0,i.jsx)(n.li,{children:"Extract audio metadata (duration, sample rate)"}),"\n",(0,i.jsx)(n.li,{children:"Perform speech-to-text conversion"}),"\n",(0,i.jsx)(n.li,{children:"Return structured JSON response"}),"\n",(0,i.jsx)(n.li,{children:"Handle recognition errors gracefully"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Algorithm Flow"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"1. Read audio bytes from stdin\n2. Detect format (RIFF\u2192WAV, fLaC\u2192FLAC, FORM\u2192AIFF)\n3. Create BytesIO buffer\n4. Open as AudioFile\n5. Extract metadata (sample rate, duration)\n6. Record audio data\n7. Call recognizer.recognize_google(audio)\n8. Format JSON response\n9. Print to stdout\n10. Exit with appropriate code\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key Libraries"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"speech_recognition"})," - Core speech-to-text functionality"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"wave"})," - WAV file metadata extraction"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"io.BytesIO"})," - In-memory binary stream handling"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"json"})," - Response formatting"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Error Handling"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"try:\n    # Audio processing\nexcept sr.UnknownValueError:\n    # Could not understand audio\n    exit(1)\nexcept sr.RequestError:\n    # API request failed\n    exit(2)\nexcept Exception:\n    # General processing error\n    exit(3)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"4-external-service-layer",children:"4. External Service Layer"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Service"}),": Google Speech Recognition API"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Responsibilities"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Accept audio data via speech_recognition library"}),"\n",(0,i.jsx)(n.li,{children:"Perform speech-to-text conversion"}),"\n",(0,i.jsx)(n.li,{children:"Return recognized text or error"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Integration"}),": Accessed through Python's ",(0,i.jsx)(n.code,{children:"speech_recognition"})," library, which handles API communication automatically."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"data-flow-diagrams",children:"Data Flow Diagrams"}),"\n",(0,i.jsx)(n.h3,{id:"successful-request-flow",children:"Successful Request Flow"}),"\n",(0,i.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant Client as Game Client\n    participant Express as Express Server\n    participant Multer as Multer Middleware\n    participant Python as Python Script\n    participant Google as Google Speech API\n    participant Logs as Log Files\n    \n    Client->>Express: POST /upload (audio file)\n    Express->>Multer: Process multipart data\n    Multer->>Express: Return file buffer\n    Express->>Express: Extract metadata<br/>(user-agent, timestamp)\n    Express->>Python: spawn() + pipe audio to stdin\n    \n    activate Python\n    Python->>Python: Detect audio format\n    Python->>Python: Extract metadata<br/>(duration, sample rate)\n    Python->>Google: recognize_google(audio)\n    Google--\x3e>Python: Transcribed text\n    Python->>Python: Format JSON response\n    Python--\x3e>Express: stdout: JSON data\n    deactivate Python\n    \n    Express->>Express: Parse Python output\n    Express->>Express: Build response object\n    \n    alt Consent Given\n        Express->>Logs: Write request log\n    end\n    \n    Express--\x3e>Client: 200 OK + JSON response\n    Client->>Client: Parse transcription\n    Client->>Client: Map to game command\n    Client->>Client: Execute action"}),"\n",(0,i.jsx)(n.h3,{id:"error-handling-flow",children:"Error Handling Flow"}),"\n",(0,i.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant Client as Game Client\n    participant Express as Express Server\n    participant Python as Python Script\n    participant Google as Google Speech API\n    \n    Client->>Express: POST /upload (audio file)\n    \n    alt No File Attached\n        Express--\x3e>Client: 418 Error: NO_FILE\n    else File Processing Error\n        Express->>Python: spawn() + pipe audio\n        Python->>Google: recognize_google(audio)\n        Google--\x3e>Python: UnknownValueError\n        Python--\x3e>Express: JSON error + exit(1)\n        Express--\x3e>Client: 300 Error: UNKNOWN_VALUE\n    else API Request Error\n        Express->>Python: spawn() + pipe audio\n        Python->>Google: recognize_google(audio)\n        Google--\x3e>Python: RequestError\n        Python--\x3e>Express: JSON error + exit(2)\n        Express--\x3e>Client: 300 Error: REQUEST_ERROR\n    else Server Error\n        Express->>Express: Exception thrown\n        Express--\x3e>Client: 500 Error: SERVER_ERROR\n    end"}),"\n",(0,i.jsx)(n.h3,{id:"complete-use-case-aac-user-playing-game",children:"Complete Use Case: AAC User Playing Game"}),"\n",(0,i.jsx)(n.mermaid,{value:'sequenceDiagram\n    participant AAC as AAC Device\n    participant User as User\n    participant Game as Game Client\n    participant API as Express API\n    participant Python as Python Script\n    participant Speech as Google Speech API\n    \n    User->>AAC: Select "Apple" on AAC board\n    AAC->>AAC: Generate speech output\n    AAC->>Game: Audio output captured\n    \n    Game->>Game: Record audio file\n    Game->>API: POST /upload<br/>(audio file + metadata)\n    \n    activate API\n    API->>API: Validate file\n    API->>API: Extract request metadata\n    API->>Python: spawn(speech2.py)<br/>+ pipe audio buffer\n    \n    activate Python\n    Python->>Python: Read from stdin\n    Python->>Python: Detect format (WAV)\n    Python->>Python: Extract duration, sample rate\n    Python->>Speech: recognize_google(audio)\n    Speech--\x3e>Python: "apple"\n    Python->>Python: Format JSON response\n    Python--\x3e>API: stdout: JSON with "apple"\n    deactivate Python\n    \n    API->>API: Parse Python output\n    API->>API: Build comprehensive response\n    API--\x3e>Game: 200 OK + JSON<br/>{transcription: "apple", ...}\n    deactivate API\n    \n    Game->>Game: Parse response\n    Game->>Game: Match "apple" to game options<br/>["apple", "papaya", "pineapple"]\n    Game->>Game: Execute selectFruit("apple")\n    Game->>User: Display "You chose: Apple"'}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"class-diagrams",children:"Class Diagrams"}),"\n",(0,i.jsx)(n.h3,{id:"express-server-architecture",children:"Express Server Architecture"}),"\n",(0,i.jsx)(n.mermaid,{value:"classDiagram\n    class ExpressApp {\n        +express app\n        +int PORT\n        +multer upload\n        +string LOG_DIR\n        +listen()\n        +use(middleware)\n    }\n    \n    class RequestHandler {\n        +handleUpload(req, res)\n        -validateFile(req)\n        -extractMetadata(req)\n        -spawnPythonProcess(audioBuffer)\n        -parseResponse(stdout)\n        -buildResponse(data)\n        -handleError(error)\n    }\n    \n    class MetadataParser {\n        +parseUserAgent(userAgent)\n        -detectBrowser(ua)\n        -detectDevice(ua)\n        +extractRequestInfo(req)\n    }\n    \n    class LoggingService {\n        +logRequest(data, consent)\n        -checkConsent(req)\n        -getLogFile(date)\n        -appendLog(file, entry)\n    }\n    \n    class PythonInterface {\n        +spawn(scriptPath)\n        +writeToStdin(buffer)\n        +readFromStdout()\n        +handleExit(code)\n    }\n    \n    ExpressApp --\x3e RequestHandler\n    RequestHandler --\x3e MetadataParser\n    RequestHandler --\x3e LoggingService\n    RequestHandler --\x3e PythonInterface"}),"\n",(0,i.jsx)(n.h3,{id:"python-script-architecture",children:"Python Script Architecture"}),"\n",(0,i.jsx)(n.mermaid,{value:"classDiagram\n    class SpeechRecognizer {\n        +sr.Recognizer recognizer\n        +processAudio()\n        -readFromStdin()\n        -detectFormat()\n        -extractMetadata()\n        -performRecognition()\n        -formatResponse()\n    }\n    \n    class AudioProcessor {\n        +detectFormat(bytes)\n        +extractDuration(audioFile)\n        +extractSampleRate(audioFile)\n        +createAudioFile(bytes)\n    }\n    \n    class ResponseFormatter {\n        +formatSuccess(text, metadata)\n        +formatError(errorType, message)\n        +toJSON(data)\n    }\n    \n    class ErrorHandler {\n        +handleUnknownValue()\n        +handleRequestError(exception)\n        +handleProcessingError(exception)\n    }\n    \n    SpeechRecognizer --\x3e AudioProcessor\n    SpeechRecognizer --\x3e ResponseFormatter\n    SpeechRecognizer --\x3e ErrorHandler"}),"\n",(0,i.jsx)(n.h3,{id:"client-integration-pattern",children:"Client Integration Pattern"}),"\n",(0,i.jsx)(n.mermaid,{value:"classDiagram\n    class GameClient {\n        +requestInput()\n        +uploadAudio(file)\n        +handleResponse(response)\n        +executeCommand(transcription)\n    }\n    \n    class AudioCapture {\n        +captureFromMicrophone()\n        +captureFromAAC()\n        +createAudioFile()\n    }\n    \n    class APIClient {\n        +post(endpoint, data)\n        +parseResponse(response)\n        +handleError(error)\n    }\n    \n    class CommandMapper {\n        +mapTranscription(text, validCommands)\n        +findBestMatch(text, options)\n        +executeGameAction(command)\n    }\n    \n    class UIController {\n        +displayResult(action)\n        +showError(message)\n        +updateGameState(state)\n    }\n    \n    GameClient --\x3e AudioCapture\n    GameClient --\x3e APIClient\n    GameClient --\x3e CommandMapper\n    GameClient --\x3e UIController"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"database-design",children:"Database Design"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Current Status"}),": The API does not use a traditional database. Instead, it implements ",(0,i.jsx)(n.strong,{children:"file-based logging"})," for request tracking and analytics."]}),"\n",(0,i.jsx)(n.h3,{id:"log-file-structure",children:"Log File Structure"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Location"}),": ",(0,i.jsx)(n.code,{children:"Initial_API/logs/"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Format"}),": JSON array per daily log file"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Filename Pattern"}),": ",(0,i.jsx)(n.code,{children:"requests-YYYY-MM-DD.json"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Schema"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'[\n  {\n    "timestamp": "2025-11-10T14:23:45.123Z",\n    "success": true,\n    "transcription": "apple",\n    "audio": {\n      "filename": "recording.wav",\n      "size": 44032,\n      "format": "WAV",\n      "duration": 2.5,\n      "sampleRate": 44100\n    },\n    "request": {\n      "timestamp": "2025-11-10T14:23:43.000Z",\n      "device": "Desktop",\n      "browser": "Chrome",\n      "userAgent": "Mozilla/5.0..."\n    },\n    "user": {\n      "id": "user123"\n    },\n    "audioBufferSize": 44032,\n    "ipAddress": "::1"\n  }\n]\n'})}),"\n",(0,i.jsx)(n.h3,{id:"future-database-considerations",children:"Future Database Considerations"}),"\n",(0,i.jsx)(n.p,{children:"For production deployments requiring analytics, user tracking, or historical data analysis, consider:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Recommended Database"}),": MongoDB or PostgreSQL"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Proposed Schema"}),":"]}),"\n",(0,i.jsx)(n.mermaid,{value:"erDiagram\n    USER ||--o{ REQUEST : makes\n    REQUEST ||--|| AUDIO_FILE : contains\n    REQUEST ||--o| ERROR : may_have\n    \n    USER {\n        string id PK\n        string session_id\n        datetime created_at\n        datetime last_seen\n        boolean consent_given\n    }\n    \n    REQUEST {\n        string id PK\n        string user_id FK\n        datetime timestamp\n        string device\n        string browser\n        string user_agent\n        string ip_address\n        boolean success\n    }\n    \n    AUDIO_FILE {\n        string id PK\n        string request_id FK\n        string filename\n        int size_bytes\n        string format\n        float duration\n        int sample_rate\n        string transcription\n    }\n    \n    ERROR {\n        string id PK\n        string request_id FK\n        string error_code\n        string error_message\n        int python_exit_code\n    }"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"algorithms-and-processing-logic",children:"Algorithms and Processing Logic"}),"\n",(0,i.jsx)(n.h3,{id:"1-audio-format-detection-algorithm",children:"1. Audio Format Detection Algorithm"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Location"}),": ",(0,i.jsx)(n.code,{children:"speech2.py"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Identify audio format from file header bytes"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def detect_format(audio_bytes):\n    """\n    Detect audio format from magic bytes (file signature)\n    """\n    if audio_bytes[:4] == b\'RIFF\':\n        return "WAV"\n    elif audio_bytes[:4] == b\'fLaC\':\n        return "FLAC"\n    elif audio_bytes[:4] == b\'FORM\':\n        return "AIFF"\n    else:\n        return "WAV"  # Default fallback\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Magic Bytes Reference"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["WAV: ",(0,i.jsx)(n.code,{children:"52 49 46 46"})," (RIFF)"]}),"\n",(0,i.jsxs)(n.li,{children:["FLAC: ",(0,i.jsx)(n.code,{children:"66 4C 61 43"})," (fLaC)"]}),"\n",(0,i.jsxs)(n.li,{children:["AIFF: ",(0,i.jsx)(n.code,{children:"46 4F 52 4D"})," (FORM)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-user-agent-parsing-algorithm",children:"2. User Agent Parsing Algorithm"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Location"}),": ",(0,i.jsx)(n.code,{children:"index.js"})," - ",(0,i.jsx)(n.code,{children:"parseUserAgent()"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Extract browser and device information from user-agent string"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"function parseUserAgent(userAgent) {\n    if (!userAgent) return { browser: 'Unknown', device: 'Unknown' };\n    \n    const ua = userAgent.toLowerCase();\n    let browser = 'Unknown';\n    let device = 'Unknown';\n    \n    // Browser detection (order matters - check specific before generic)\n    if (ua.includes('edg')) browser = 'Edge';\n    else if (ua.includes('chrome')) browser = 'Chrome';\n    else if (ua.includes('firefox')) browser = 'Firefox';\n    else if (ua.includes('safari')) browser = 'Safari';\n    else if (ua.includes('opera')) browser = 'Opera';\n    \n    // Device detection\n    if (ua.includes('mobile') || ua.includes('android') || ua.includes('iphone')) {\n        device = 'Mobile';\n    } else if (ua.includes('tablet') || ua.includes('ipad')) {\n        device = 'Tablet';\n    } else {\n        device = 'Desktop';\n    }\n    \n    return { browser, device };\n}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-speech-recognition-pipeline",children:"3. Speech Recognition Pipeline"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Location"}),": ",(0,i.jsx)(n.code,{children:"speech2.py"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Library"}),": ",(0,i.jsx)(n.code,{children:"speech_recognition"})," (Google Speech Recognition)"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Process"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# 1. Initialize recognizer\nrecognizer = sr.Recognizer()\n\n# 2. Load audio from bytes\naudio_file = io.BytesIO(audio_bytes)\n\n# 3. Open as AudioFile context\nwith sr.AudioFile(audio_file) as source:\n    # 4. Record audio data\n    audio = recognizer.record(source)\n\n# 5. Perform recognition\ntext = recognizer.recognize_google(audio)\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Training Data"}),": Uses Google's pre-trained speech recognition models (black box, not customizable in free tier)"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Accuracy Factors"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Audio quality (sample rate, bit depth)"}),"\n",(0,i.jsx)(n.li,{children:"Background noise levels"}),"\n",(0,i.jsx)(n.li,{children:"Speaker clarity and accent"}),"\n",(0,i.jsx)(n.li,{children:"Audio duration (optimal: 1-30 seconds)"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"4-consent-based-logging-algorithm",children:"4. Consent-Based Logging Algorithm"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Location"}),": ",(0,i.jsx)(n.code,{children:"index.js"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Purpose"}),": Determine if request should be logged based on consent"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"function shouldLog(req) {\n    // Explicit consent via header\n    if (req.get('x-logging-consent') === 'true') {\n        return true;\n    }\n    \n    // Explicit consent via body\n    if (req.body.loggingConsent === true) {\n        return true;\n    }\n    \n    // Auto-consent in development (not production)\n    if (process.env.NODE_ENV !== 'production') {\n        return true;\n    }\n    \n    // Default: no logging\n    return false;\n}\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"1-input-validation",children:"1. Input Validation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"File presence check"}),": Returns 418 error if no file uploaded"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Format validation"}),": Python script validates audio file headers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Size limits"}),": Configurable via Multer (currently unlimited, should be restricted in production)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-data-privacy",children:"2. Data Privacy"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Consent-based logging"}),": No data stored without user permission"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IP anonymization"}),": Consider hashing IP addresses before logging"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GDPR compliance"}),": User data can be deleted by removing log files"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-api-security",children:"3. API Security"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"CORS enabled"}),": Restricts which domains can access the API (configure allowed origins in production)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"No authentication"}),": Currently open API (add API keys or OAuth for production)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Rate limiting"}),": Not implemented (recommended for production)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"4-subprocess-security",children:"4. Subprocess Security"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fixed Python script path"}),": No user-controlled script execution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Stdin-only communication"}),": No file system access from user input"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Process isolation"}),": Python subprocess runs independently"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"1-memory-usage",children:"1. Memory Usage"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"In-memory buffers"}),": Multer stores files in RAM (faster but limited by server memory)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Streaming"}),": Audio data streamed to Python via stdin (no disk I/O)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Garbage collection"}),": Buffers released after request completion"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-concurrency",children:"2. Concurrency"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Async I/O"}),": Node.js handles multiple concurrent requests"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Subprocess pooling"}),": Each request spawns new Python process (consider process pooling for high traffic)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Non-blocking"}),": Server remains responsive during Python processing"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-response-times",children:"3. Response Times"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Typical"}),": 2-5 seconds for 10-20 second audio clips"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Factors"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Audio duration"}),"\n",(0,i.jsx)(n.li,{children:"Google API latency"}),"\n",(0,i.jsx)(n.li,{children:"Network speed"}),"\n",(0,i.jsx)(n.li,{children:"Server load"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"deployment-architecture",children:"Deployment Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"development-environment",children:"Development Environment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Developer Machine             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Node.js (localhost:8080)        \u2502\n\u2502 Python 3.x                      \u2502\n\u2502 Test audio files                \u2502\n\u2502 Git repository                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h3,{id:"production-environment-recommended",children:"Production Environment (Recommended)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Load Balancer  \u2502\n\u2502   (nginx/ALB)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502         \u2502\n\u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510\n\u2502 App 1 \u2502 \u2502 App 2 \u2502  (Express + Python)\n\u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n    \u2502         \u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Log Storage    \u2502\n\u2502  (S3 / EFS)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsxs)(n.p,{children:["The AAC Integration API implements a ",(0,i.jsx)(n.strong,{children:"three-tier architecture"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Client Layer"}),": Game applications that capture audio and send HTTP requests"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Server Layer"}),": Express.js API that handles uploads, manages subprocesses, and formats responses"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Processing Layer"}),": Python script that performs speech-to-text conversion via Google's API"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["The system is designed to be ",(0,i.jsx)(n.strong,{children:"stateless"}),", ",(0,i.jsx)(n.strong,{children:"scalable"}),", and ",(0,i.jsx)(n.strong,{children:"privacy-conscious"}),", with comprehensive error handling and metadata tracking. The modular design allows for future enhancements such as real-time streaming, custom speech models, and direct AAC board integration."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key Architectural Decisions"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"RESTful API for simplicity and broad compatibility"}),"\n",(0,i.jsx)(n.li,{children:"Subprocess architecture for language flexibility (Node.js + Python)"}),"\n",(0,i.jsx)(n.li,{children:"File-based logging for development simplicity"}),"\n",(0,i.jsx)(n.li,{children:"Consent-based data collection for privacy compliance"}),"\n",(0,i.jsx)(n.li,{children:"Comprehensive metadata for debugging and analytics"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);